In this section we collect some recommendations for constructing convolutional networks as well as efficiently training deep neural networks. These are intended to be specific to this project but most will also be useful in similar projects.
% Some of these details are taken care by the software(either as a defualt or optional feature) and some are qyuite new and need to be taken care manually.
%technical concepts
% the reasoning behind is explained on the references sources. 

%mamogrpahy images are sufficiently different from images used in other object recognition challenges for example labeling may not be as sharp, they are not color mages, the object to recognize is quite small compared to the image, ammograms come in different sizes and the usable part is rectangular (higher than wider), mammogrpahuics image are sometimes required to identify the object and localize it which is not a requirement on other data. and thus some of these advice may prove counterproductive. We will start with the most weel tested features, diagnose the model and add features which willl hopefully produce beter vresults and work this way until obtaining the best model which will be trained and tested for a final time.

\paragraph{Image preprocessing} Some standard processing for images.
\begin{itemize}
	\item Images are cropped to contain only the relevant parts of the image, denoised, enhanced and optionally downsampled to maintain the input size manageable.
	\item Each image feature (the raw pixels) is zero centered by substracting its mean across all training images. Normalization scales the already zero-centered features to range from $[-1 \dots 1]$ by dividing them by its standard deviations. Feature normalization is not striclty neccesary but still customary~\cite{Karpathy2015}.
	\item The test data should not be used to calculate any statistic used to preprocess the training data. Furthermore, these same statistics (calculated from the training data) should later be used when normalizing the test data~\cite{Karpathy2015}.
\end{itemize}

\paragraph{Convolutional network architecture}
We offer some guidelines for designing convolutional network architectures and some standard values for various hyperparameters.

\begin{itemize}
	\item It is always better to select a complex network architecture which is flexible enough to model the data and manage overfitting with regularization rather than 
	an architecture which is not powerful enough to model the data~\cite{Ng2014, Krizhevsky2012}. 

	\item Although, theoretically, neural networks with a single hidden layer are universal approximators provided they have enough units ($\mathcal{O}(2^n)$ where $n$ is the size of the input), in practice, deeper architectures produce better results using less units overall. This insigth holds for convolutional networks~\cite{Bengio2014}.

	\item As a rule of thumb for big datasets, use 10-20 layers overall for convolutional networks. For smaller datasets, use less layers or transfer learning~\cite{Karpathy2015}.
	% How am I counting the number of layers, does the 10-20 refers to onlly fully connected and conv, did i get this number from Karpathy or from  engio(i think bengio). "for big datasets" and big image size, too or not?

	\item Use the number of parameters rather than the number of layers or units as a measure of the architecture's complexity.

	\item Use 2-3 \texttt{CONV -> RELU} pairs before pooling (N above)~\cite{Karpathy2015}. Pooling is a destructive operation and having two convolutional layers together allows them to pick up more complex features.

	\item Use one or more . This number dependsThe number of \texttt{COnvRELu POOL} depends on the complexity of the features expected in the data (and the size of the data. . In a way, this is the actual number of layers of a convolutional network. This also decdeds how much is the volume subsampled.
	% Rewrite. 

	\item Use less than 3 \text{FC -> RELU} pairs before the output layer (K above)~\cite{Karpathy2015}. When the volume arrives to the fully connected layers it has shrinken enough and using more fully connected layers risks overfitting.






	\item Use 2 by2 pooling. Both non-overlapping and overlapping pooling produce similar resultsKarpathy, Krizhevsky.overlapping pools give better results in Krizhevsky

	\item Prefer small filter size on the convolutions (3by3 or 5by5)(cite Striving for simplicity or Lin 2014). If using pooling use one of the two common pooling mechanisms, becaus eusing biggest filters would be too agressive and throw away much of the information computed by the network.
	\item Stride: using stride of 1 is standard (for small filters) with zero padding in where needed to assure the dimension of the feature map is maintained (for instance, padding one column at each side of the matrix with a 3 by 3 filter will produce an image of the same width, similarly for rows. For filter size 5, padding of 2 will work), if using a bigger stride convolutional networks can be made to soimulate poooling layers as explained in Section ...
	use 3by3 filters with dtride 2 and zero-padding 1 or 5by5 with stride 1  and zero padding 2. This preserves the spatial dimanesions of the volume and work better in practice. Bigger filters are used for big images and normally only on the first convolutional layers compromise to use a bigger filter because the parameters of a small filter on a very big image is unbvearable. 

	\item the input layer should have dimensions divisible by 2 and usually squared images. this is needed so that we can use 2-by-2 pooling layers which divide the image by 2 in each dimension.

	\item convetrt fully connected layers into convolutional layers.

	\item Number of feature maps starts per convolutional layer small and grows with time. (?) VGA has the same number at each feature map, ...it depends it is like the number of units in anetwork or number of different features per layer. alex Net increases because as the layer increases there are more compelx features to be learned than at the start plus  as the feature maps get smaller (becaus of pooling) is more computationally feasible to have more of them. Vga net simplifies it by using al convoutional net uses. 
	yeah, Zoo and all convolutional do the same, feature maps increase. 

	\item The number of feature maps on fully connected layers or equivalently the number of units in the fully connected layer, decreases  layer by layer. For instance the first fully connected layer could have 2048 units, the second 512 and teh final layer 32 outputs. They are normally a intermediate number between the number of classes (which is smaller) and the number of units in the last convolutional layer (number of units in each feature map multiplied by the number of feature maps). They decrease from the number of units in the last convolutional layer (the dimensions of the volume multiplied) until the number of units in he output layer. For instance for a convolutional netowkr with two fully connected layers and 10 possible classes if the lat convolutional layer is a volume of size $8 \times 8 \times 512$ (8192 units), the first fully connected network could have size 2048 and the second (the output) size 10.
\end{itemize}


\begin{comment}
\paragraph{Choosing hyperparameters}
Other hyperparameters rather tahn those of the architecture and training

\item the important ones are. Others will report only a slight improvement into 

\item The complete list of hyperparameters

Hyperparameters: Learning rate very important, parameter initialization somehow important, number of hidden units (use many plus regularization), L1 may be a better regularizer
Hyperparameter fitting: preprocessing, learning rate, lambda. size of the neural network (just big). Use one validation. Search in log scale. random search. momentum = [0.5, 0.9, 0.95, 0.99] or 0.9
Hyperparameters: preprocessing(4), small vs big image, hidden layers, hidden units, regularization param, learning parameter, mu(momentum), p(dropout),
Learning rate: Cross'validate to choose the best one. Use exponential scales: 0.1, 0.01, .001. also plot error/loss vs epochs, choos ethe one that converges fast or gives the better learning rate.
hyperparameter search random Bergstra and Bengio



\paragraph{Dealing with overfitting}
t is normally recommended to have a flexible model and convtrol for ovefittting. this can be done by various means. plus regularization.


Dat augmentation:
in order to generate additional examples from tehe data
 flipping over x, training the network on smaller pathces selected from the original image (a similar approach is used in testing to made predictions), selecting only smaller patches of the original image, rotations, and adding random noise to the image (jittering the colors)(). Most of this do not need to be stored explicitly but can be generated during training. 
Data augmenattion rotations and flipping over axis (Krhizhevsky, as in GalaxyZoo and Lo et al.)

Dropout(hinton et al., 2012): averaging over many mehtods.
Regularization: l2-norm normally with Dropout(Srivastava et al, 2014), works like that, doe sthat and that. p=0.5, scale the activations. Only subsample a neural network to copute the output and make an update. Inverted dropout recomended. Hyperpaameters: p, lambda. 

\textbf{Training}:

Randomize trainig set before training

initialization: For ReLu each weight vector w = np.random.randn(n) * sqrt(2.0/n) (He et al, 2015). 0 bias initialization or small random numbers do not really matter.

Use minibatch stochastic gradient descent. No point on using the exact gradient because you ar enot going to advance much and it may not even be the right direction.Minibatch gradient descent. 256 examles.>100 Stochastc gradient descent is extreme with oine example.

Early stopping. free lunch, #of iterations per training should not be thought as ahyperparameter. You can just rusn until it strts overfitting or it doesn{t improve anymore and preserve the best fitted Ã¡rameters. SO stop the execution every #of steps (multiple of validation set size) to see what{s the training error and how well will it do. Maybe juts set max number of iterations. important print error to see what's ging one. Use heuristic as stopping criteria, for example if I did x #iter to get to my best parameters (say 100) and in another #iter I have not improved it, then  it probably converged (so it will stop in 200).

Learning: sgd+momentum, with nag, Decreasing/decaying learning rate schedule: slowly(divided by two every number of steps)(do i need this?, maybe not).

\paragraph{Other things to try}
For transfer learning, given that a network trained in the ImageNet is very diffferent to the images we are going to test them on and our dtaaset is small/large, we should train a linear clasifier from the activations somewhere inside of the network (where the features obtained are probably more general)/ we could fine tune only the upper layers of the network. Plus when finetuning the learning rate is normally smaller that wehne training a network from scratch. \item Transfer Learning may not work cause the dataset is quite different.
\item Have a score for every space or apply the convolutional network to different parts of the input (this needs way too many parameters).
\item all convolutional 

\textbf{Checks}:
Overfit a tiny subset of data(cost=0). If i can't is not worthede going to a bigger dataset.
Gradient check, loss

\textbf{Unbalanced data}:
Change the threshold for unbalanced classes. Cross validate it to see which one is better, never use the test set. Explain why using one metric and not the other.
%Make sure I said that data replication (rotations and such) is needed because e don't have enough examples of a simple class, not because we are trying to balance the classes. Data replication is different.

\paragraph{Software} 
\item caffe
\item theano,
\item torch 
\item deeplearning4j
\end{comment}

%Deep Learning: (https://www.youtube.com/watch?v=JuimBuvEWBg)
%Bengio2013(techreport): Practical reccomendations for gradient based training of deep architecthure
%Activation function: although the sigmoid function is historically used it has fal out of favor... (http://cs231n.github.io/neural-networks-1/) because in the end of the outputs it is plabne and its graident \emph{vanish} (i.e., become 0), thus special care is needed when initializing it to otherwise it will not learn. Second drawback, it is not zero-centered (not so important, the first one is). Tanh is always preferred to the sigmoid non-linearity. ReLUs for the win (can die, need good learning rate)., maxouts(put formula, double the number of weights per neuron).

% Train a simple convolutional
% Train one with bells and whistles(choosen by heart)
% Improve on the two,
% how to deal with the small dataset (how small it is)
